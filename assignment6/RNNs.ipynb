{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UZQ9Tm1AK1V"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P59NYU98GCb9",
        "outputId": "bccaf90c-1319-4de5-a20a-7acca802ab41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==0.4.1 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==0.4.1\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 16.0 MB 4.3 MB/s \n",
            "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires bokeh<2.4.0,>=2.3.0, but you have bokeh 0.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 5.4 MB 3.7 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.20.2 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.20.2 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8sVtGHmA9aBM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiA2dGmgF1rW",
        "outputId": "c541f753-590c-4ebf-e397-5acd28309e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dtype=np.int):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, positive=False):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  precompute=False, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, random_state=None,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QstS4NO0L97c",
        "outputId": "7f751a47-581c-4738-e756-3e4aa506ac98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ]
        }
      ],
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTai8Ta0lgwL",
        "outputId": "4e0ade1d-2b45-4330-8904-044eb5468c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSYFdoOPAySI",
        "outputId": "08ceef1c-0836-4745-96c6-2f1eb3257c46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('``', '.'),\n",
              " ('What', 'DET'),\n",
              " ('did', 'VERB'),\n",
              " ('you', 'PRON'),\n",
              " ('think', 'VERB'),\n",
              " ('about', 'ADP'),\n",
              " (\"Bang-Jensen's\", 'NOUN'),\n",
              " ('contention', 'NOUN'),\n",
              " ('of', 'ADP'),\n",
              " ('errors', 'NOUN'),\n",
              " ('and', 'CONJ'),\n",
              " ('omissions', 'NOUN'),\n",
              " ('in', 'ADP'),\n",
              " ('the', 'DET'),\n",
              " ('Hungarian', 'ADJ'),\n",
              " ('report', 'NOUN'),\n",
              " (\"''\", '.'),\n",
              " ('?', '.'),\n",
              " ('?', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCjwwDs6Zq9x",
        "outputId": "d253cebc-9e68-4182-8577-ef8c3ff57fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in train = 45441. Tags = {'CONJ', 'X', 'PRT', 'ADJ', 'DET', '.', 'ADV', 'PRON', 'NUM', 'ADP', 'VERB', 'NOUN'}\n"
          ]
        }
      ],
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "URC1B2nvPGFt",
        "outputId": "9337159b-fa35-4fa0-a253-0e523cb76bbb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rWmSToIaeAo",
        "outputId": "37343f76-08c3-422e-d12c-3ee0307e465b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjz_Rk0bbMyH",
        "outputId": "1e2db9e7-85a1-4217-c7fa-da35723804da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ]
        }
      ],
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XCuxEBVbOY_",
        "outputId": "623801e8-e0da-4cfc-aee0-1265e34bf490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ]
        }
      ],
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RtRbz1SwgEqc"
      },
      "outputs": [],
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DhsTKZalfih6"
      },
      "outputs": [],
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4XsRII5kW5x",
        "outputId": "3040e0d7-e459-4284-c0c4-94eae519e7f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WVEHju54d68T"
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.tagset_size = tagset_size\n",
        "        self.lstm_layers_count = lstm_layers_count\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
        "        self.fc = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        embedded_inputs = self.embedding(inputs)\n",
        "        output, _ = self.lstm(embedded_inputs)\n",
        "        output = self.fc(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jbrxsZ2mehWB"
      },
      "outputs": [],
      "source": [
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GMUyUm1hgpe3"
      },
      "outputs": [],
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(X_batch)\n",
        "\n",
        "def compute_accuracy(pred, target):\n",
        "    _, indices = torch.max(pred, -1)\n",
        "    num_total = torch.sum(target>0).item()\n",
        "    num_cor = torch.sum((indices == target)*(target>0)).item()\n",
        "    return num_cor, num_total\n",
        "\n",
        "compute_accuracy(logits, y_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pon0YD09BfI7",
        "outputId": "30b90a43-d235-4c0b-bb5e-2d2000868333"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17, 92)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "loss(logits.reshape((-1,len(tag2ind))), y_batch.view(-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JGsFWN7BiTL",
        "outputId": "82aaa8aa-4664-4530-cec8-41ece9bdb81c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.4978, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FprPQ0gllo7b"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch).cuda(), LongTensor(y_batch).cuda()\n",
        "                \n",
        "                logits = model(X_batch)\n",
        "                loss = criterion(logits.reshape((-1, len(tag2ind))), y_batch.view(-1))\n",
        "                epoch_loss += loss.item()\n",
        "                \n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                cur_correct_count, cur_sum_count = compute_accuracy(logits, y_batch)\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqfbeh1ltEYa",
        "outputId": "612eaca0-a631-473a-9180-47296011226a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 10] Train: Loss = 0.68398, Accuracy = 78.29%: 100%|██████████| 572/572 [00:15<00:00, 37.53it/s]\n",
            "[1 / 10]   Val: Loss = 0.35933, Accuracy = 88.27%: 100%|██████████| 13/13 [00:00<00:00, 38.69it/s]\n",
            "[2 / 10] Train: Loss = 0.27257, Accuracy = 90.97%: 100%|██████████| 572/572 [00:16<00:00, 35.28it/s]\n",
            "[2 / 10]   Val: Loss = 0.24726, Accuracy = 92.33%: 100%|██████████| 13/13 [00:00<00:00, 37.55it/s]\n",
            "[3 / 10] Train: Loss = 0.18536, Accuracy = 93.87%: 100%|██████████| 572/572 [00:15<00:00, 37.31it/s]\n",
            "[3 / 10]   Val: Loss = 0.20140, Accuracy = 93.87%: 100%|██████████| 13/13 [00:00<00:00, 37.54it/s]\n",
            "[4 / 10] Train: Loss = 0.13846, Accuracy = 95.42%: 100%|██████████| 572/572 [00:15<00:00, 36.64it/s]\n",
            "[4 / 10]   Val: Loss = 0.18408, Accuracy = 94.54%: 100%|██████████| 13/13 [00:00<00:00, 38.82it/s]\n",
            "[5 / 10] Train: Loss = 0.10811, Accuracy = 96.38%: 100%|██████████| 572/572 [00:15<00:00, 38.02it/s]\n",
            "[5 / 10]   Val: Loss = 0.17259, Accuracy = 94.99%: 100%|██████████| 13/13 [00:00<00:00, 38.57it/s]\n",
            "[6 / 10] Train: Loss = 0.08612, Accuracy = 97.11%: 100%|██████████| 572/572 [00:15<00:00, 37.84it/s]\n",
            "[6 / 10]   Val: Loss = 0.17032, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 37.30it/s]\n",
            "[7 / 10] Train: Loss = 0.06925, Accuracy = 97.66%: 100%|██████████| 572/572 [00:15<00:00, 38.07it/s]\n",
            "[7 / 10]   Val: Loss = 0.17770, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 38.07it/s]\n",
            "[8 / 10] Train: Loss = 0.05609, Accuracy = 98.12%: 100%|██████████| 572/572 [00:15<00:00, 37.99it/s]\n",
            "[8 / 10]   Val: Loss = 0.18423, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 36.90it/s]\n",
            "[9 / 10] Train: Loss = 0.04606, Accuracy = 98.46%: 100%|██████████| 572/572 [00:15<00:00, 37.98it/s]\n",
            "[9 / 10]   Val: Loss = 0.19249, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 39.29it/s]\n",
            "[10 / 10] Train: Loss = 0.03759, Accuracy = 98.75%: 100%|██████████| 572/572 [00:15<00:00, 37.39it/s]\n",
            "[10 / 10]   Val: Loss = 0.20053, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 32.60it/s]\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=10,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "98wr38_rw55D"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data, batch_size):\n",
        "    loss, acc = do_epoch(model, criterion, data, batch_size, None, 'Eval:')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, data=(X_test, y_test), batch_size=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwHcXdrpB4pL",
        "outputId": "efb1d8f4-0789-4d6a-a4f8-2d445f89ebbc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Eval: Loss = 0.19867, Accuracy = 95.54%: 100%|██████████| 28/28 [00:00<00:00, 33.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uZpY_Q1xZ18h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsCstxiO03oT",
        "outputId": "019bc122-b831-4357-ba3f-85b2123a8263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ]
        }
      ],
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "LxaRBpQd0pat"
      },
      "outputs": [],
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.tagset_size = tagset_size\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.lstm_layers_count = lstm_layers_count\n",
        "        \n",
        "        self.num_embeddings, self.embedding_dim = embeddings.shape\n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "        self.embedding.load_state_dict({'weight': torch.tensor(embeddings)})\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, lstm_hidden_dim, lstm_layers_count, bidirectional = True)\n",
        "        self.fc = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = self.fc(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBtI6BDE-Fc7",
        "outputId": "2a4110fd-d50d-4b1c-bb1e-9de488a93d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 10] Train: Loss = 0.40227, Accuracy = 88.59%: 100%|██████████| 572/572 [00:13<00:00, 43.16it/s]\n",
            "[1 / 10]   Val: Loss = 0.11494, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 44.60it/s]\n",
            "[2 / 10] Train: Loss = 0.07236, Accuracy = 97.77%: 100%|██████████| 572/572 [00:13<00:00, 43.56it/s]\n",
            "[2 / 10]   Val: Loss = 0.08616, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 44.74it/s]\n",
            "[3 / 10] Train: Loss = 0.04649, Accuracy = 98.52%: 100%|██████████| 572/572 [00:13<00:00, 43.66it/s]\n",
            "[3 / 10]   Val: Loss = 0.08142, Accuracy = 97.33%: 100%|██████████| 13/13 [00:00<00:00, 42.01it/s]\n",
            "[4 / 10] Train: Loss = 0.03384, Accuracy = 98.92%: 100%|██████████| 572/572 [00:13<00:00, 43.39it/s]\n",
            "[4 / 10]   Val: Loss = 0.07891, Accuracy = 97.48%: 100%|██████████| 13/13 [00:00<00:00, 43.96it/s]\n",
            "[5 / 10] Train: Loss = 0.02630, Accuracy = 99.15%: 100%|██████████| 572/572 [00:13<00:00, 43.53it/s]\n",
            "[5 / 10]   Val: Loss = 0.08301, Accuracy = 97.38%: 100%|██████████| 13/13 [00:00<00:00, 43.69it/s]\n",
            "[6 / 10] Train: Loss = 0.02083, Accuracy = 99.34%: 100%|██████████| 572/572 [00:13<00:00, 43.49it/s]\n",
            "[6 / 10]   Val: Loss = 0.08836, Accuracy = 97.32%: 100%|██████████| 13/13 [00:00<00:00, 44.79it/s]\n",
            "[7 / 10] Train: Loss = 0.01671, Accuracy = 99.46%: 100%|██████████| 572/572 [00:13<00:00, 43.51it/s]\n",
            "[7 / 10]   Val: Loss = 0.08916, Accuracy = 97.38%: 100%|██████████| 13/13 [00:00<00:00, 42.42it/s]\n",
            "[8 / 10] Train: Loss = 0.01319, Accuracy = 99.59%: 100%|██████████| 572/572 [00:13<00:00, 43.46it/s]\n",
            "[8 / 10]   Val: Loss = 0.10015, Accuracy = 97.13%: 100%|██████████| 13/13 [00:00<00:00, 46.61it/s]\n",
            "[9 / 10] Train: Loss = 0.01026, Accuracy = 99.69%: 100%|██████████| 572/572 [00:13<00:00, 43.41it/s]\n",
            "[9 / 10]   Val: Loss = 0.10592, Accuracy = 97.10%: 100%|██████████| 13/13 [00:00<00:00, 44.82it/s]\n",
            "[10 / 10] Train: Loss = 0.00789, Accuracy = 99.77%: 100%|██████████| 572/572 [00:13<00:00, 43.40it/s]\n",
            "[10 / 10]   Val: Loss = 0.11637, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 45.01it/s]\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=10,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPUuAPGhEGVR",
        "outputId": "890b9e6e-1d04-4972-aef8-32d280d748ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Eval: Loss = 0.11694, Accuracy = 97.01%: 100%|██████████| 28/28 [00:00<00:00, 39.79it/s]\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, data=(X_test, y_test), batch_size=512)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RNNs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}